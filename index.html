<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-MYG8Y8V6GQ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag("js", new Date());

    gtag("config", "G-MYG8Y8V6GQ");
  </script>
  <title>Xintao Wang</title>
  <!-- Meta -->
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description" content="" />
  <meta name="author" content="Xiaoying Riley at 3rd Wave Media" />
  <link rel="shortcut icon" href="images/daodao_draw_180.jpg" />

  <link href="https://fonts.googleapis.com/css?family=Lato:300,400,300italic,400italic" rel="stylesheet"
    type="text/css" />
  <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css" />

  <!-- FontAwesome JS -->
  <script defer src="assets/fontawesome/js/all.js"></script>

  <!-- Global CSS -->
  <link rel="stylesheet" href="assets/plugins/bootstrap/css/bootstrap.min.css" />

  <!-- github calendar css -->
  <!-- <link rel="stylesheet" href="assets/plugins/github-calendar/dist/github-calendar-responsive.css"> -->
  <!-- github activity css -->
  <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/octicons/2.0.2/octicons.min.css"> -->
  <!-- <link rel="stylesheet" href="assets/plugins/github-activity/src/github-activity.css"> -->

  <!-- Theme CSS -->
  <link id="theme-style" rel="stylesheet" href="assets/css/styles.css" />
</head>

<body>
  <!-- ******HEADER****** -->
  <header class="header">
    <div class="container">
      <div class="row align-items-center">
        <div class="col">
          <!-- <img class="profile-image img-fluid float-start rounded-circle" src="images/me2.jpg" alt="profile image" /> -->
          <img class="profile-image img-fluid float-start rounded-circle" src="images/me2.jpg" alt="profile image"
            onmouseover="this.src='images/daodao_draw_180.jpg'" onmouseout="this.src='images/me2.jpg'" />
          <div class="profile-content">
            <h1 class="name">Xintao Wang</h1>
            <!-- <h2 class="desc">Web App Developer</h2> -->
            <ul class="social list-inline">
              <li class="list-inline-item">
                <a href="https://twitter.com/xinntao" target="_blank"><i class="fab fa-twitter"></i></a>
              </li>
              <!-- <li class="list-inline-item"><a href="#"><i class="fab fa-linkedin-in"></i></a></li> -->
              <li class="list-inline-item">
                <a href="https://scholar.google.com.hk/citations?user=FQgZpQoAAAAJ&hl=en" target="_blank"><i
                    class="fab fa-google"></i></a>
              </li>
              <li class="list-inline-item">
                <a href="https://github.com/xinntao" target="_blank"><i class="fab fa-github"></i></a>
              </li>
              <li class="list-inline-item">
                <a href="email.html" target="_blank"><i class="fas fa-envelope-square"></i></a>
              </li>
              <li class="list-inline-item last-item">
                <a href="https://www.zhihu.com/people/xintao-28" target="_blank"><i class="fab fa-zhihu"></i></a>
              </li>
              <li class="list-inline-item last-item">
                <a href="https://xinntao.notion.site/" target="_blank">
                  <div class="icon-with-text">
                    <i class="fa fa-fw"></i>
                    <span class="text">Notion</span>
                  </div>
                </a>
              </li>
              <li class="list-inline-item last-item">
                <a href="https://www.yuque.com/xinntao/nm1yxs" target="_blank">
                  <div class="icon-with-text">
                    <i class="fa fa-fw"></i>
                    <span class="text">语雀</span>
                  </div>
                </a>
              </li>
              <!-- <li class="list-inline-item last-item"><a href="https://www.yuque.com/xinntao" target="_blank"><i class="fab fa-yuque"></i></a></li> -->
            </ul>
          </div>
          <!--//profile-->
        </div>
        <!--//col-->
        <div class="col-12 col-md-auto">
          <div class="dark-mode-switch d-flex">
            <div class="form-check form-switch mx-auto mx-md-0">
              <!-- <input type="checkbox" class="form-check-input me-2" id="darkSwitch" checked/> -->
              <input type="checkbox" class="form-check-input me-2" id="darkSwitch" />
              <label class="custom-control-label" for="darkSwitch">Dark Mode</label>
            </div>
          </div>
          <!--//dark-mode-switch-->
          <a class="btn btn-cta-primary" href="email.html" target="_blank"><i class="fas fa-paper-plane"></i> Contact
            Me</a>
        </div>
        <!--//col-->
      </div>
      <!--//row-->
    </div>
    <!--//container-->
  </header>
  <!--//header-->

  <div class="container sections-wrapper py-5">
    <div class="row">
      <div class="primary col-lg-8 col-12">
        <section class="about section">
          <div class="section-inner shadow-sm rounded">
            <!-- <h2 class="heading">About Me</h2> -->
            <div class="content">
              <!-- https://github.com/KwaiVGI -->
              I am currently a Senior Staff Researcher at KwaiVGI,
              <a href="https://www.kuaishou.com/en" target="_blank">Kuaishou Technology</a>, leading an effort on
              <b>multimodal generation</b>, including
              image/<b>video generation</b> and 3D generation. Previously, I
              was a Senior Staff Researcher at
              <a href="https://arc.tencent.com/" target="_blank">Tencent ARC Lab</a>
              and
              <a href="https://www.tencent.com/en-us/" target="_blank">Tencent AI Lab</a>.<br />
              <font color="orange"><b>We are actively looking for research interns and full-time
                  researchers to work on cutting-edge research topics</b></font>. If you're interested in exploring
              these opportunities, please
              reach out to me at
              <font color="orange"><a href="mailto:xintao.wang@outlook.com">xintao.wang@outlook.com</a></font>.<br />
              <p align="left">
                I received my Ph.D. from
                <a href="http://mmlab.ie.cuhk.edu.hk/" target="_blank">Multimedia Lab (MMLab)</a>,
                <a href="https://www.cuhk.edu.hk/english/index.html" target="_blank">the Chinese University of Hong
                  Kong</a>, advised by Prof.
                <a href="https://scholar.google.com/citations?user=qpBtpGsAAAAJ" target="_blank">Xiaoou Tang</a>
                and Prof.
                <a href="http://personal.ie.cuhk.edu.hk/~ccloy/" target="_blank">Chen Change Loy</a>. I obtained my
                bachelor's degree from
                <a href="https://www.zju.edu.cn/english/" target="_blank">Zhejiang University</a>.
              </p>
            </div>
            <!--//content-->
          </div>
          <!--//section-inner-->
        </section>
        <!--//section-->

        <!-- <section class="about section">
                    <div class="section-inner shadow-sm rounded">
                        <div class="content">
                            <p align="left">
                            I am currently immersed in the exhilarating field of <font color="orange"><b>generative AI</b></font>, which has been an exciting journey.<br>

                            <font color="orange"><b>● 2D (Image/Video) Generation</b></font>
                            <ul>
                                <li><b>Controllable Image Generation</b>: T2I-Adapter, PhotoMaker, CustomNet, MasaCtrl, DragonDiffusion, SmartEdit</li>
                                <li><b>Controllable Video Generation</b>: MotionCtrl, Tune-A-Video</li>
                                <li><b>Video Foundation Models </b>: VideoCrafter Sereries (VideoCrafter1, DynamiCrafter, EvalCrafter, StyleCrafter, etc).</li>
                            </ul>
                            <font color="orange"><b>● 3D Generation</b></font>
                            <ul>
                                <li>Dream3D, GET3D——</li>
                            </ul>
                            ● Previously, I worked on <b><font color="orange">Restoration</b></font>
                            <ul>
                                <li><b>General Image  Restoration</b>: Real-ESRGAN, ESRGAN</li>
                                <li><b>Face Restoration</b>: GFPGAN, VQFR, GLEAN</li>
                                <li><b>Video Restoration </b>: EDVR, BasicVSR</li>
                                <li><b>Training Frameworks and others </b>: BasicSR, SFTGAN</li>
                            </ul>
                        </div>
                    </div>
                </section> -->
        <!--=====================================================  News  ====================================================-->
        <section class="projects section">
          <div class="section-inner shadow-sm rounded">
            <h2 class="heading">News</h2>
            <div class="item-content">
              <ul class="resume-list" style="list-style: outside; list-style-type: square">
                <li>
                  <b>[3/2025]</b> We release several interesting papers: <a href="https://github.com/KwaiVGI/ReCamMaster"
                    target="_blank">ReCamMaster</a>, <a href="https://github.com/KwaiVGI/DiffMoE"
                    target="_blank">DiffMoE</a> and <a href=""
                    target="_blank">IGV-as-GGE</a>.
                </li>
                <li>
                  <b>[2/2025]</b> Three papers (<a href="https://zixuan-ye.github.io/stylemaster/"
                    target="_blank">StyleMaster</a>, PatchVSR and
                  SketchVideo) are
                  accepted to CVPR 2025.
                </li>
                <li>
                  <b>[1/2025]</b> Two papers (<a href="https://jianhongbai.github.io/SynCamMaster/"
                    target="_blank">SynCamMaster</a> and
                  <a href="https://fuxiao0719.github.io/projects/3dtrajmaster/" target="_blank">3DTrajMaster</a>) are
                  accepted to ICLR 2025.
                </li>
                <li>
                  <b>[12/2024]</b> Three papers (<a href="https://liyaowei-stu.github.io/project/ImageConductor/"
                    target="_blank">ImageConductor</a>,
                  <a href="https://customcrafter.github.io/" target="_blank">CustomCrafter</a> and
                  Anti-Diffusion[<b>oral</b>]) are accepted to AAAI 2025.
                </li>
                <li>
                  <b>[9/2024]</b> Ranked as
                  <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/7" target="_blank">Top 2%
                    Scientists Worldwide 2024</a>
                  (Single Year) by Stanford University.
                </li>
                <li>
                  <b>[9/2024]</b> Three papers (<a href="https://mira-space.github.io/" target="_blank">MiraData</a>, <a
                    href="https://mc-e.github.io/project/ReVideo/" target="_blank">ReVideo</a> and
                  <a href="https://videotetris.github.io/" target="_blank">VideoTetris</a>) are accepted to NeurIPS
                  2024.
                </li>
                <li>
                  <b>[9/2024]</b> One paper (<a href="https://arxiv.org/abs/2309.01770"
                    target="_blank">StyleAdapter</a>) is accepted to
                  IJCV.
                </li>
                <li>
                  <b>[8/2024]</b> Two papers (<a href="https://doubiiu.github.io/projects/ToonCrafter/"
                    target="_blank">ToonCrafter</a> and <a href="https://gongyeliu.github.io/StyleCrafter.github.io/"
                    target="_blank">StyleCrafter</a>)
                  are accepted to SIGGRAPH Asia 2024.
                </li>
                <li>
                  <b>[7/2024]</b> One paper (<a href="https://jiangyzy.github.io/CustomNet/"
                    target="_blank">CustomNet</a>) is accepted to ACM MM
                  2024.
                </li>
                <li>
                  <b>[7/2024]</b> Five papers (<a href="https://doubiiu.github.io/projects/DynamiCrafter/"
                    target="_blank">DynamiCrafter</a>[<b>oral</b>],
                  <a href="https://tencentarc.github.io/BrushNet/" target="_blank">BrushNet</a>, <a
                    href="https://github.com/bbaaii/DreamDiffusion" target="_blank">DreamDiffusion</a>, <a
                    href="https://myniuuu.github.io/MOFA_Video/" target="_blank">MOFA-Video</a>, <a
                    href="https://guolanqing.github.io/Self-Cascade/" target="_blank">CheapScaling</a>) are accepted to
                  ECCV 2024.
                </li>
                <li>
                  <b>[5/2024]</b> One paper (<a href="https://github.com/lyh-18/PromptGIP"
                    target="_blank">PromptGIP</a>) is accepted to ICML
                  2024.
                </li>
                <li>
                  <b>[3/2024]</b> One paper (<a href="https://wzhouxiff.github.io/projects/MotionCtrl"
                    target="_blank">MotionCtrl</a>) is accepted to
                  SIGGRAPH 2024.
                </li>
                <li>
                  <b>[3/2024]</b> Nine papers (<a href="https://github.com/TencentARC/PhotoMaker"
                    target="_blank">PhotoMaker</a>, <a href="https://github.com/Fanghua-Yu/SUPIR"
                    target="_blank">SUPIR</a>, <a href="https://ailab-cvc.github.io/videocrafter2/"
                    target="_blank">VideoCrafter2</a>, <a href="https://yuzhou914.github.io/SmartEdit/"
                    target="_blank">SmartEdit</a>[<b>highlight</b>], <a href="https://arxiv.org/abs/2212.03185"
                    target="_blank">Rethink VQ Tokenizer</a>, <a href="https://evalcrafter.github.io/"
                    target="_blank">EvalCrafter</a>, <a href="https://showlab.github.io/X-Adapter/"
                    target="_blank">X-Adapter</a>, <a href="https://arxiv.org/abs/2402.02583"
                    target="_blank">DiffEditor</a> and <a href="https://yzxing87.github.io/Seeing-and-Hearing/"
                    target="_blank">Seeing&Hearing</a>) are accepted to CVPR 2024.
                </li>
                <li>
                  <b>[1/2024]</b> Four papers (<a href="https://mc-e.github.io/project/DragonDiffusion/"
                    target="_blank">DragonDiffusion</a>[<b>spotlight</b>], <a
                    href="https://yingqinghe.github.io/scalecrafter/"
                    target="_blank">ScaleCrafter</a>[<b>spotlight</b>], <a
                    href="http://haonanqiu.com/projects/FreeNoise.html" target="_blank">FreeNoise</a> and <a
                    href="https://ailab-cvc.github.io/seed/seed_llama.html" target="_blank">SEED-LlaMa</a>) are accepted
                  to ICLR 2024.
                </li>
                <li>
                  <b>[12/2023]</b> Three papers are accepted to AAAI 2024.
                </li>
                <li>
                  <b>[10/2023]</b> Ranked as
                  <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/6" target="_blank">Top 2%
                    Scientists Worldwide 2023</a>
                  (Single Year) by Stanford University.
                </li>

                <details>
                  <summary>Click for More</summary>
                  <li>
                    <b>[10/2023]</b> Two papers are accepted to NeurIPS 2023.
                  </li>
                  <li>
                    <b>[09/2023]</b> Release
                    <a href="https://github.com/TencentARC/T2I-Adapter" target="_blank">T2I-Adapter</a>
                    for SDXL: the most efficient control models, collaborating
                    with
                    <a href="https://huggingface.co/blog/t2i-sdxl-adapters" target="_blank">HuggingFace</a>.
                  </li>
                  <li>
                    <b>[07/2023]</b> Three papers are accepted to ICCV 2023.
                  </li>
                  <li><b>[04/2023]</b> One paper is accepted to ICML 2023.</li>
                  <li>
                    <b>[03/2023]</b> We are holding the
                    <a href="https://github.com/360SR/360SR-Challenge" target="_blank">360° Super-Resolution
                      Challenge</a>
                    as a part of the
                    <a href="https://cvlai.net/ntire/2023/" target="_blank">NTIRE workshop</a>
                    in conjunction with CVPR 2023.
                  </li>
                  <li><b>[02/2023]</b> Three papers to appear in CVPR 2023.</li>
                  <li><b>[11/2022]</b> Two papers to appear in AAAI 2023.</li>
                  <li>
                    <b>[09/2022]</b> Ranked as
                    <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/5" target="_blank">Top 2%
                      Scientists Worldwide 2022</a>
                    (Single Year) by Stanford University.
                  </li>
                  <li>
                    <b>[09/2022]</b> Two papers to appear in NeurIPS 2022.
                  </li>
                  <li>
                    <b>[07/2022]</b> Two papers to appear in ECCV 2022. VQFR is
                    accepted as <font color="orange">oral</font> (2.7%).
                  </li>
                  <li><b>[06/2022]</b> Two papers to appear in ACM MM 2022.</li>
                  <li>
                    <b>[05/2022]</b> BasicSR joins the
                    <a href="https://github.com/XPixelGroup" target="_blank">XPixel Group</a>!
                  </li>
                  <li>
                    <b>[04/2022]</b> We release a high-quality face video
                    dataset (VFHQ). Please refer to the
                    <a href="https://liangbinxie.github.io/projects/vfhq">project page</a>
                    and
                    <a href="https://arxiv.org/abs/2205.03409">our paper</a>.
                  </li>
                  <li>
                    <b>[12/2021]</b> One paper to appear in NeurIPS 2021 as
                    <font color="orange">spotlight</font> (2.85%):
                    <a
                      href="https://proceedings.neurips.cc/paper/2021/hash/008bd5ad93b754d500338c253d9c1770-Abstract.html">FAIG:
                      Finding Discriminative Filters for Specific
                      Degradations in Blind Super-Resolution</a>. Codes are released in
                    <a href="https://github.com/TencentARC/FAIG">TencentARC/FAIG</a>.
                  </li>
                  <li>
                    <b>[10/2021]</b>
                    <a href="https://arxiv.org/abs/2107.10833">Real-ESRGAN</a>
                    is accepted by ICCV 2021 AIM workshop with Honorary
                    Nomination Paper Award.
                  </li>
                  <li>
                    [07/2021] One paper to appear in ICCV 2021:
                    <a href="https://arxiv.org/abs/2108.08826">Towards Vivid and Diverse Image Colorization with
                      Generative Color Prior</a>
                  </li>
                  <li>
                    [07/2021] The codes for practical image restoration
                    <a href="https://arxiv.org/abs/2107.10833">Real-ESRGAN</a>
                    are released on
                    <a href="https://github.com/xinntao/Real-ESRGAN">Github</a>.
                  </li>
                  <li>
                    [06/2021] The training and testing codes of GFPGAN are
                    released on
                    <a href="https://github.com/TencentARC/GFPGAN">TencentARC</a>.
                  </li>
                  <li>[03/2021] 5 papers to appear in CVPR 2021.</li>
                  <li>
                    [03/2021] A brand-new
                    <a href="https://github.com/xinntao/HandyView">HandyView</a>
                    online!.
                  </li>
                  <li>
                    [08/2020] A brand-new
                    <a href="https://github.com/xinntao/BasicSR">BasicSR</a>
                    v1.0.0 online!
                  </li>
                  <li>
                    [06/2019] We have released the
                    <a href="https://github.com/xinntao/EDVR">EDVR</a>
                    training and testing codes and also updated
                    <a href="https://github.com/xinntao/BasicSR">BasicSR</a>
                    codes!
                  </li>
                  <li>
                    [06/2019] Got my first outstanding reviewer recognition
                    from CVPR 2019!
                  </li>
                  <li>
                    [05/2019] Our video restoration method, <b>EDVR</b>, won
                    all four tracks in the
                    <a href="http://www.vision.ee.ethz.ch/ntire19/">NTIRE 2019 video restoration and enhancement
                      challenges</a>. Check
                    <a href="https://arxiv.org/abs/1905.02716">our paper</a>
                    for more details.
                  </li>
                  <li>
                    [03/2019] Our paper
                    <a href="https://xinntao.github.io/projects/DNI"><i>Deep Network Interpolation for Continuous
                        Imagery
                        Effect Transition</i></a>
                    to appear in CVPR 2019.
                  </li>
                  <li>
                    [08/2018] Our SuperSR team won the third track of the
                    <a href="https://www.pirm2018.org/PIRM-SR.html">2018 PIRM Challenge on Perceptual
                      Super-Resolution</a>. Check the report
                    <a href="https://arxiv.org/abs/1809.00219"><i>ESRGAN</i></a>
                    for more details.
                  </li>
                  <li>
                    [06/2018] We won the
                    <a href="http://www.vision.ee.ethz.ch/ntire17/NTIRE">NTIRE 2018 Challenge on Single Image
                      Super-Resolution</a>
                    as first runner-up and ranked the first in the
                    <i>Realistic Wild ×4 conditions</i>
                    track.
                  </li>
                  <li>
                    [02/2018] Our paper
                    <a href="http://mmlab.ie.cuhk.edu.hk/projects/SFTGAN/"><i>Recovering Realistic Texture in Image
                        Super-resolution by Deep Spatial Feature Transform</i></a>
                    to appear in CVPR 2018.
                  </li>
                  <li>
                    [07/2017] Our HelloSR team won the
                    <a href="http://www.vision.ee.ethz.ch/ntire17/NTIRE">NTIRE 2017 Challenge on Single Image
                      Super-Resolution</a>
                    as first runner-up.
                  </li>
                </details>
              </ul>
            </div>
            <!--//content-->
          </div>
          <!--//section-inner-->
        </section>
        <!--//section-->

        <!--=====================================================  Selected Open-Sources  ====================================================-->
        <section class="cards-section text-center">
          <!-- <div class="section-inner shadow-sm rounded"> -->
          <div class="section-inner rounded">
            <div id="cards-wrapper" class="cards-wrapper row">
              <!------------------------------------------ GFPGAN ----------------------------------->
              <div class="item item-purple col-lg-4 col-6">
                <div class="item-inner">
                  <div class="icon-holder">
                    <!-- <span aria-hidden="true" class="icon icon_datareport_alt"></span> -->
                  </div>
                  <!--//icon-holder-->
                  <h3 class="title">
                    GFPGAN
                    <img alt="GitHub stars" src="https://img.shields.io/github/stars/TencentARC/GFPGAN?style=social" />
                  </h3>
                  <p class="intro">Practical face restoration</p>
                  <a class="link" href="https://github.com/TencentARC/GFPGAN" target="_blank"><span></span></a>
                </div>
                <!--//item-inner-->
              </div>
              <!--//item-->

              <!------------------------------------------ Real-ESRGAN ----------------------------------->
              <div class="item item-blue item-2 col-lg-4 col-6">
                <div class="item-inner">
                  <div class="icon-holder">
                    <!-- <span aria-hidden="true" class="icon icon_puzzle_alt"></span> -->
                  </div>
                  <!--//icon-holder-->
                  <h3 class="title">
                    Real-ESRGAN
                    <img alt="GitHub stars"
                      src="https://img.shields.io/github/stars/xinntao/Real-ESRGAN?style=social" />
                  </h3>
                  <p class="intro">
                    Practical algorithms for image restoration
                  </p>
                  <a class="link" href="https://github.com/xinntao/Real-ESRGAN" target="_blank"><span></span></a>
                </div>
                <!--//item-inner-->
              </div>
              <!--//item-->

              <!------------------------------------------ BasicSR ----------------------------------->
              <div class="item item-primary col-lg-4 col-6">
                <div class="item-inner">
                  <div class="icon-holder">
                    <!-- <span aria-hidden="true" class="icon icon_lifesaver"></span> -->
                  </div>
                  <!--//icon-holder-->
                  <h3 class="title">
                    BasicSR
                    <img alt="GitHub stars"
                      src="https://img.shields.io/github/stars/XPixelGroup/BasicSR?style=social" />
                  </h3>
                  <p class="intro">
                    Open source image and video restoration toolbox
                  </p>
                  <a class="link" href="https://github.com/XPixelGroup/BasicSR" target="_blank"><span></span></a>
                </div>
                <!--//item-inner-->
              </div>
              <!--//item-->

              <!------------------------------------------ T2I-Adapter ----------------------------------->
              <div class="item item-pink col-lg-4 col-6">
                <div class="item-inner">
                  <div class="icon-holder">
                    <!-- <i class="icon fa fa-paper-plane"></i> -->
                  </div>
                  <!--//icon-holder-->
                  <h3 class="title">
                    T2I-Adapter
                    <img alt="GitHub stars"
                      src="https://img.shields.io/github/stars/TencentARC/T2I-Adapter?style=social" />
                  </h3>
                  <p class="intro">
                    Dig out controllable ability for text-to-image diffusion
                    models
                  </p>
                  <a class="link" href="https://github.com/TencentARC/T2I-Adapter" target="_blank"><span></span></a>
                </div>
                <!--//item-inner-->
              </div>
              <!--//item-->

              <!------------------------------------------ VideoCrafter ----------------------------------->
              <div class="item item-green col-lg-4 col-6">
                <div class="item-inner">
                  <div class="icon-holder">
                    <!-- <span aria-hidden="true" class="icon icon_gift"></span> -->
                  </div>
                  <!--//icon-holder-->
                  <h3 class="title">
                    VideoCrafter
                    <img alt="GitHub stars"
                      src="https://img.shields.io/github/stars/AILab-CVC/VideoCrafter?style=social" />
                  </h3>
                  <p class="intro">
                    Open sourced large models for video generation
                  </p>
                  <a class="link" href="https://github.com/AILab-CVC/VideoCrafter" target="_blank"><span></span></a>
                </div>
                <!--//item-inner-->
              </div>
              <!--//item-->
              <!------------------------------------------ HandyView ----------------------------------->
              <div class="item item-orange col-lg-4 col-6">
                <div class="item-inner">
                  <div class="icon-holder">
                    <!-- <span aria-hidden="true" class="icon icon_genius"></span> -->
                  </div>
                  <!--//icon-holder-->
                  <h3 class="title">
                    HandyView
                    <img alt="GitHub stars" src="https://img.shields.io/github/stars/xinntao/HandyView?style=social" />
                  </h3>
                  <p class="intro">Handy image viewer</p>
                  <a class="link" href="https://github.com/xinntao/HandyView" target="_blank"><span></span></a>
                </div>
                <!--//item-inner-->
              </div>
              <!--//item-->
            </div>
            <!--//cards-->
          </div>
          <!--//container-->
        </section>
        <!--//cards-section-->

        <!--=====================================================  Publications  ====================================================-->
        <section class="latest section">
          <div class="section-inner shadow-sm rounded">
            <h2 class="heading">
              Publications
              <a href="https://scholar.google.com.hk/citations?user=FQgZpQoAAAAJ&hl=en" target="_blank">[Full List]</a>
            </h2>
            <div class="content">
              <small>(* equal contribution, <sup>#</sup> corresponding
                author)</small>
              <br />
              <!-- <hr class="divider" /> -->

              <!------------------------------------------ Template ----------------------------------->
              <!-- <div class="item row">
                <a class="col-md-4 col-12" href="https://jianhongbai.github.io/SynCamMaster/" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/syncammaster.jpg" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h4 class="title">
                    <a href="https://arxiv.org/abs/2412.07760" target="_blank">
                      <font color="orange">SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse
                        Viewpoints</font>
                    </a>
                  </h4>
                  <p class="mb-2">
                    Jiwen Yu, Yiran Qin, <b>Xintao Wang<sup>#</sup></b>, Pengfei Wan, Di Zhang, Xihui Liu<sup>#</sup>
                  </p>
                  <p>arXiv preprint: 2412.07760</br>
                    ICLR, 2025. &nbsp;
                    <a class="more-link" href="https://jianhongbai.github.io/SynCamMaster/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;<a class="more-link"
                      href="https://arxiv.org/abs/2412.07760" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/KwaiVGI/SynCamMaster" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/KwaiVGI/SynCamMaster" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/KwaiVGI/SynCamMaster?style=social" /></a>
                  </p>
                </div>
              </div> -->

              <h4 class="title">
                <font color="#49ac43">Selected Preprint</font>
              </h4>
              <!------------------------------------------ ReCamMaster ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://jianhongbai.github.io/ReCamMaster/" target="_blank">
                  <video autoplay controls="" muted loop playsinline="" width="100%" style="width: 260px">
                    <source src="https://jianhongbai.github.io/ReCamMaster/videos/ReCamMaster/TEASER_compressed.mp4" type="video/mp4" />
                  </video>
                </a>
                <div class="desc col-md-8 col-12">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2503.11647" target="_blank">
                      <font color="orange">ReCamMaster: Camera-Controlled Generative Rendering from A Single Video</font>
                    </a>
                  </h3>
                  <p class="mb-2">
                    Jianhong Bai, Menghan Xia, Xiao Fu, <b>Xintao Wang</b>, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, Di Zhang
                  </p>
                  <p>
                    arXiv preprint: 2503.11647. </br>
                    &nbsp;
                    <a class="more-link" href="https://jianhongbai.github.io/ReCamMaster/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2503.11647" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/KwaiVGI/ReCamMaster" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/KwaiVGI/ReCamMaster" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/KwaiVGI/ReCamMaster?style=social" /></a>
                  </p>
                </div>
              </div>
               <!------------------------------------------ DiffMoE ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://shiml20.github.io/DiffMoE/" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="https://shiml20.github.io/DiffMoE/static/images/teaser.png" alt="teaser" />
                </a>
                </a>
                <div class="desc col-md-8 col-12">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2503.14487" target="_blank">
                      <font color="orange">DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers</font>
                    </a>
                  </h3>
                  <p class="mb-2">
                    Minglei Shi, Ziyang Yuan, Haotian Yang, <b>Xintao Wang<sup>#</sup></b>, Mingwu Zheng, Xin Tao, Wenliang Zhao, Wenzhao Zheng, Jie Zhou, Jiwen Lu<sup>#</sup>, Pengfei Wan, Di Zhang, Kun Gai
                  </p>
                  <p>
                    arXiv preprint: 2503.14487. </br>
                    &nbsp;
                    <a class="more-link" href="https://shiml20.github.io/DiffMoE/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2503.14487" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/KwaiVGI/DiffMoE" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/KwaiVGI/DiffMoE" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/KwaiVGI/DiffMoE?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ VideoAlign ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://gongyeliu.github.io/videoalign" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/videoalign.jpg" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2501.13918" target="_blank">
                      <font color="orange">Improving Video Generation with Human Feedback</font>
                    </a>
                  </h3>
                  <p class="mb-2">
                    Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang,
                    Wenyu Qin, Menghan Xia, <b>Xintao Wang</b>, Xiaohong Liu, Fei Yang, Pengfei Wan, Di Zhang, Kun Gai,
                    Yujiu Yang, Wanli Ouyang
                  </p>
                  <p>
                    arXiv preprint: 2501.13918. </br>
                    &nbsp;
                    <a class="more-link" href="https://gongyeliu.github.io/videoalign" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2501.13918" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a
                        class="more-link"
                        href="https://github.com/KwaiVGI/VideoAlign"
                        target="_blank"
                        ><i class="fab fa-github"></i>Codes</a
                      >&nbsp;
                      <a
                        class="more-link"
                        href="https://github.com/KwaiVGI/VideoAlign"
                        target="_blank"
                        ><img
                          alt="GitHub stars"
                          align="right"
                          src="https://img.shields.io/github/stars/KwaiVGI/VideoAlign?style=social"
                      /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ GameFactory ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://vvictoryuki.github.io/gamefactory" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/gamefactory.jpg" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2501.08325" target="_blank">
                      <font color="orange">GameFactory: Creating New Games with Generative Interactive Videos</font>
                    </a>
                  </h3>
                  <p class="mb-2">
                    Jiwen Yu, Yiran Qin, <b>Xintao Wang<sup>#</sup></b>, Pengfei Wan, Di Zhang, Xihui Liu<sup>#</sup>
                  </p>
                  <p>
                    arXiv preprint: 2501.08325. </br>
                    &nbsp;
                    <a class="more-link" href="https://vvictoryuki.github.io/gamefactory" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2501.08325" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/KwaiVGI/GameFactory" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/KwaiVGI/GameFactory" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/KwaiVGI/GameFactory?style=social" /></a>
                  </p>
                </div>
              </div>

              <!-- ################################ 2024 ################################# -->
              <h4 class="title">
                <font color="#49ac43">2024</font>
              </h4>
              <!------------------------------------------ SynCamMaster ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://jianhongbai.github.io/SynCamMaster/" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/syncammaster.jpg" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h4 class="title">
                    <a href="https://arxiv.org/abs/2412.07760" target="_blank">
                      <font color="orange">SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse
                        Viewpoints</font>
                    </a>
                  </h4>
                  <p class="mb-2">
                    Jianhong Bai, Menghan Xia, <b>Xintao Wang</b>, Ziyang Yuan, Xiao Fu, Zuozhu Liu, Haoji Hu, Pengfei
                    Wan, Di Zhang
                  </p>
                  <p>arXiv preprint: 2412.07760</br>
                    ICLR, 2025. &nbsp;
                    <a class="more-link" href="https://jianhongbai.github.io/SynCamMaster/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;<a class="more-link"
                      href="https://arxiv.org/abs/2412.07760" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/KwaiVGI/SynCamMaster" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/KwaiVGI/SynCamMaster" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/KwaiVGI/SynCamMaster?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ StyleMaster ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://zixuan-ye.github.io/stylemaster/" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/stylemaster.jpg" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h4 class="title">
                    <a href="https://arxiv.org/abs/2412.07744" target="_blank">StyleMaster: Stylize Your Video with
                      Artistic Generation and Translation</a>
                  </h4>
                  <p class="mb-2">
                    Zixuan Ye, Huijuan Huang, <b>Xintao Wang</b>, Pengfei Wan, Di Zhang, Wenhan Luo
                  </p>
                  <p>arXiv preprint: 2412.07744</br>
                    CVPR, 2025. &nbsp;
                    <a class="more-link" href="https://zixuan-ye.github.io/stylemaster/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;<a class="more-link"
                      href="https://arxiv.org/abs/2412.07744" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/KwaiVGI/StyleMaster" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/KwaiVGI/StyleMaster" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/KwaiVGI/StyleMaster?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ 3DTrajMaster ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://fuxiao0719.github.io/projects/3dtrajmaster" target="_blank">
                  <video autoplay controls="" muted loop playsinline="" width="100%" style="width: 260px">
                    <source src="./images/3dtrajmaster.mp4" type="video/mp4" />
                  </video>
                </a>
                <div class="desc col-md-8 col-12">
                  <h4 class="title">
                    <a href="https://arxiv.org/abs/2412.07759" target="_blank">
                      <font color="orange">3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video
                        Generation</font>
                    </a>
                  </h4>
                  <p class="mb-2">
                    Xiao Fu, Xian Liu, <b>Xintao Wang<sup>#</sup></b>, Sida Peng, Menghan Xia, Xiaoyu Shi, Ziyang Yuan,
                    Pengfei Wan, Di Zhang, Dahua Lin<sup>#</sup>
                  </p>
                  <p>arXiv preprint: 2412.07759</br>
                    ICLR, 2025. &nbsp;
                    <a class="more-link" href="https://fuxiao0719.github.io/projects/3dtrajmaster/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;<a class="more-link"
                      href="https://arxiv.org/abs/2412.07759" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/KwaiVGI/3DTrajMaster" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/KwaiVGI/3DTrajMaster" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/KwaiVGI/3DTrajMaster?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ MiraData ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://mira-space.github.io/" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/miradata.jpg" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h4 class="title">
                    <a href="https://arxiv.org/abs/2407.06358" target="_blank">MiraData: A Large-Scale Video Dataset
                      with Long Durations and Structured Captions</a>
                  </h4>
                  <p class="mb-2">
                    Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, <b>Xintao Wang</b>, Ailing Zeng, Yu Xiong, Qiang
                    Xu, Ying Shan
                  </p>
                  <p>arXiv preprint: 2407.06358</br>
                    NeurIPS (Datasets & Benchmarks Track), 2024.&nbsp;
                    <a class="more-link" href="https://mira-space.github.io/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;<a class="more-link"
                      href="https://arxiv.org/abs/2407.06358" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/mira-space/MiraData" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/mira-space/MiraData" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/mira-space/MiraData?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ MOFA-Video ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://myniuuu.github.io/MOFA_Video/" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/mofa.jpg" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h4 class="title">
                    <a href="https://arxiv.org/abs/2405.20222" target="_blank">MOFA-Video: Controllable Image Animation
                      via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model</a>
                  </h4>
                  <p class="mb-2">
                    Muyao Niu, Xiaodong Cun, <b>Xintao Wang</b>, Yong Zhang, Ying Shan, Yinqiang Zheng
                  </p>
                  <p>arXiv preprint: 2405.20222</br>
                    ECCV, 2024. &nbsp;
                    <a class="more-link" href="https://myniuuu.github.io/MOFA_Video/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;<a class="more-link"
                      href="https://arxiv.org/abs/2405.20222" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/MyNiuuu/MOFA-Video" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/MyNiuuu/MOFA-Video" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/MyNiuuu/MOFA-Video?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ ToonCrafter ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://doubiiu.github.io/projects/ToonCrafter/" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/tooncrafter.jpg" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h4 class="title">
                    <a href="https://arxiv.org/abs/2405.17933" target="_blank">
                      <font color="orange">ToonCrafter: Generative Cartoon
                        Interpolation</font>
                    </a>
                  </h4>
                  <p class="mb-2">
                    Jinbo Xing, Hanyuan Liu, Menghan Xia, Yong Zhang, <b>Xintao Wang</b>, Ying Shan, Tien-Tsin Wong
                  </p>
                  <p>arXiv preprint: 2405.17933</br>
                    SIGGRAPH Asia, 2024. &nbsp;
                    <a class="more-link" href="https://doubiiu.github.io/projects/ToonCrafter/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;<a class="more-link"
                      href="https://arxiv.org/abs/2405.17933" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/Doubiiu/ToonCrafter" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/Doubiiu/ToonCrafter" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/Doubiiu/ToonCrafter?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ ReVideo ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://mc-e.github.io/project/ReVideo/" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/miradata.jpg" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h4 class="title">
                    <a href="https://arxiv.org/abs/2405.13865" target="_blank">ReVideo: Remake a Video with Motion and
                      Content Control</a>
                  </h4>
                  <p class="mb-2">
                    Chong Mou, Mingdeng Cao, <b>Xintao Wang<sup>#</sup></b>, Zhaoyang Zhang, Ying Shan, Jian
                    Zhang<sup>#</sup>
                  </p>
                  <p>arXiv preprint: 2405.13865</br>
                    NeurIPS, 2024.&nbsp;
                    <a class="more-link" href="https://mc-e.github.io/project/ReVideo/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;<a class="more-link"
                      href="https://arxiv.org/abs/2405.13865" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/MC-E/ReVideo" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/MC-E/ReVideo" target="_blank"><img alt="GitHub stars"
                        align="right" src="https://img.shields.io/github/stars/MC-E/ReVideo?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ BrushNet ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://tencentarc.github.io/BrushNet/" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/brushnet.jpg" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h4 class="title">
                    <a href="https://arxiv.org/abs/2403.06976" target="_blank">BrushNet: A Plug-and-Play Image
                      Inpainting Model with Decomposed Dual-Branch Diffusion</a>
                  </h4>
                  <p class="mb-2">
                    Xuan Ju, Xian Liu, <b>Xintao Wang<sup>#</sup></b>, Yuxuan Bian, Ying Shan, Qiang Xu<sup>#</sup>
                  </p>
                  <p>arXiv preprint: 2403.06976</br>
                    ECCV, 2024. &nbsp;
                    <a class="more-link" href="https://tencentarc.github.io/BrushNet/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;<a class="more-link"
                      href="https://arxiv.org/abs/2403.06976" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/BrushNet" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/BrushNet" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/TencentARC/BrushNet?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ SUPIR ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://github.com/Fanghua-Yu/SUPIR" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/supir.jpg" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h4 class="title">
                    <a href="https://arxiv.org/abs/2401.13627" target="_blank">
                      <font color="orange">Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image
                        Restoration In the Wild</font>
                    </a>
                  </h4>
                  <p class="mb-2">
                    Fanghua Yu, Jinjin Gu, Zheyuan Li, Jinfan Hu, Xiangtao Kong, <b>Xintao Wang</b>, Jingwen He, Yu
                    Qiao, Chao Dong
                  </p>
                  <p>arXiv preprint: 2401.13627</br>
                    CVPR, 2024. &nbsp;
                    <a class="more-link" href="https://github.com/Fanghua-Yu/SUPIR" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;<a class="more-link"
                      href="https://arxiv.org/abs/2401.13627" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/Fanghua-Yu/SUPIR" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/Fanghua-Yu/SUPIR" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/Fanghua-Yu/SUPIR?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ VideoCrafter2 ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://ailab-cvc.github.io/videocrafter2/" target="_blank">
                  <video autoplay controls="" muted loop playsinline="" width="100%" style="width: 260px">
                    <source src="./images/videocrafter2.mp4" type="video/mp4" />
                  </video>
                </a>
                <div class="desc col-md-8 col-12">
                  <h4 class="title">
                    <a href="https://arxiv.org/abs/2401.09047" target="_blank">
                      <font color="orange">VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion
                        Models</font>
                    </a>
                  </h4>
                  <p class="mb-2">
                    Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, <b>Xintao Wang</b>, Chao Wen, Ying Shan
                  </p>
                  <p>arXiv preprint: 2401.09047</br>
                    CVPR, 2024. &nbsp;
                    <a class="more-link" href="https://ailab-cvc.github.io/videocrafter2/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;<a class="more-link"
                      href="https://arxiv.org/abs/2401.09047" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/AILab-CVC/VideoCrafter" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/AILab-CVC/VideoCrafter" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/AILab-CVC/VideoCrafter?style=social" /></a>
                  </p>
                </div>
              </div>
              <!-- ################################ 2023 ################################# -->
              <h4 class="title">
                <font color="#49ac43">2023</font>
              </h4>
              <!------------------------------------------ SmartEdit ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://yuzhou914.github.io/SmartEdit/" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/smartedit.jpg" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h4 class="title">
                    <a href="https://arxiv.org/abs/2312.06739" target="_blank">
                      SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models
                    </a>
                  </h4>
                  <p class="mb-2">
                    Yuzhou Huang, Liangbin Xie, <b>Xintao Wang<sup>#</sup></b>, Ziyang Yuan, Xiaodong Cun, Yixiao Ge,
                    Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang<sup>#</sup>, Ying Shan
                  </p>
                  <p>arXiv preprint: 2312.06739</br>
                    CVPR, 2024 (<font color="red">hilight</font>). &nbsp;
                    <a class="more-link" href="https://yuzhou914.github.io/SmartEdit/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;<a class="more-link"
                      href="https://arxiv.org/abs/2312.06739" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/SmartEdit" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/SmartEdit" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/TencentARC/SmartEdit?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ PhotoMaker ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://photo-maker.github.io/" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/photomaker.jpg" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h4 class="title">
                    <a href="https://arxiv.org/abs/2312.04461" target="_blank">
                      <font color="orange">PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding
                      </font>
                    </a>
                  </h4>
                  <p class="mb-2">
                    Zhen Li, Mingdeng Cao, <b>Xintao Wang<sup>#</sup></b>, Zhongang Qi, Ming-Ming Cheng<sup>#</sup>,
                    Ying Shan
                  </p>
                  <p>arXiv preprint: 2312.04461</br>
                    CVPR, 2024. &nbsp;
                    <a class="more-link" href="https://photo-maker.github.io/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;<a class="more-link"
                      href="https://arxiv.org/abs/2312.04461" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/PhotoMaker" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/PhotoMaker" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/TencentARC/PhotoMaker?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ MotionCtrl ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://wzhouxiff.github.io/projects/MotionCtrl" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/motionctrl.gif" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h4 class="title">
                    <a href="https://arxiv.org/abs/2312.03641" target="_blank">
                      <font color="orange">MotionCtrl: A Unified and Flexible Motion Controller for Video Generation
                      </font>
                    </a>
                  </h4>
                  <p class="mb-2">
                    Zhouxia Wang, Ziyang Yuan, <b>Xintao Wang<sup>#</sup></b>, Tianshui Chen, Menghan Xia, Ping
                    Luo<sup>#</sup>, Ying Shan
                  </p>
                  <p>arXiv preprint: 2312.03641</br>
                    SIGGRAPH, 2024. &nbsp;
                    <a class="more-link" href="https://wzhouxiff.github.io/projects/MotionCtrl" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;<a class="more-link"
                      href="https://arxiv.org/abs/2312.03641" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/MotionCtrl" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/MotionCtrl" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/TencentARC/MotionCtrl?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ StyleCrafter ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://gongyeliu.github.io/StyleCrafter.github.io/" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/stylecrafter.jpg" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h4 class="title">
                    <a href="https://arxiv.org/abs/2312.00330" target="_blank">StyleCrafter: Enhancing Stylized
                      Text-to-Video Generation with Style Adapter</a>
                  </h4>
                  <p class="mb-2">
                    Gongye Liu, Menghan Xia, Yong Zhang, Haoxin Chen, Jinbo Xing, Yibo Wang, <b>Xintao Wang</b>, Yujiu
                    Yang, Ying Shan
                  </p>
                  <p>arXiv preprint: 2312.00330</br>
                    SIGGRAPH Asia, 2024. &nbsp;
                    <a class="more-link" href="https://gongyeliu.github.io/StyleCrafter.github.io/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;<a class="more-link"
                      href="https://arxiv.org/abs/2312.00330" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/GongyeLiu/StyleCrafter" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/GongyeLiu/StyleCrafter" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/GongyeLiu/StyleCrafter?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ CustomNet ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://jiangyzy.github.io/CustomNet/" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/customnet.jpg" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h4 class="title">
                    <a href="https://arxiv.org/abs/2310.19784" target="_blank">CustomNet: Object Customization with
                      Variable-Viewpoints in Text-to-Image Diffusion Models</a>
                  </h4>
                  <p class="mb-2">
                    Ziyang Yuan, Mingdeng Cao, <b>Xintao Wang<sup>#</sup></b>, Zhongang Qi, Chun Yuan<sup>#</sup>, Ying
                    Shan
                  </p>
                  <p>arXiv preprint: 2310.19784</br>
                    ACM MM, 2024. &nbsp;
                    <a class="more-link" href="https://jiangyzy.github.io/CustomNet/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;<a class="more-link"
                      href="https://arxiv.org/abs/2310.19784" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                  </p>
                </div>
              </div>
              <!------------------------------------------ FreeNoise ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="http://haonanqiu.com/projects/FreeNoise.html" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/freenoise.jpg" alt="teaser" />
                </a>

                <div class="desc col-md-8 col-12">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2310.15169" target="_blank">FreeNoise: Tuning-Free Longer Video
                      Diffusion via Noise Rescheduling</a>
                  </h3>
                  <p class="mb-2">
                    Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, <b>Xintao Wang</b>, Ying Shan, Ziwei Liu
                  </p>
                  <p>
                    arXiv preprint: 2310.15169. <br>
                    ICLR, 2024 &nbsp;
                    <a class="more-link" href="http://haonanqiu.com/projects/FreeNoise.html" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2310.15169" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/AILab-CVC/FreeNoise" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/AILab-CVC/FreeNoise" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/AILab-CVC/FreeNoise?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ DynamiCrafter ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://doubiiu.github.io/projects/DynamiCrafter/" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/dynamicrafter.jpg"
                    alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h4 class="title">
                    <a href="https://arxiv.org/abs/2310.12190" target="_blank">
                      <font color="orange">DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors
                      </font>
                    </a>
                  </h4>
                  <p class="mb-2">
                    Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, <b>Xintao
                      Wang</b>, Ying Shan, Tien-Tsin Wong
                  </p>
                  <p>arXiv preprint: 2310.12190</br>
                    ECCV, 2024 (<font color="red">oral</font>). &nbsp;
                    <a class="more-link" href="https://doubiiu.github.io/projects/DynamiCrafter/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;<a class="more-link"
                      href="https://arxiv.org/abs/2310.12190" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/Doubiiu/DynamiCrafter" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/Doubiiu/DynamiCrafter" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/Doubiiu/DynamiCrafter?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ EvalCrafter ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://evalcrafter.github.io/" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/customnet.jpg" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h4 class="title">
                    <a href="https://arxiv.org/abs/2310.11440" target="_blank">EvalCrafter: Benchmarking and Evaluating
                      Large Video Generation Models</a>
                  </h4>
                  <p class="mb-2">
                    Yaofang Liu, Xiaodong Cun, Xuebo Liu, <b>Xintao Wang</b>, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong
                    Zeng, Raymond H. Chan, Ying Shan
                  </p>
                  <p>arXiv preprint: 2310.11440</br>
                    CVPR, 2024. &nbsp;
                    <a class="more-link" href="https://evalcrafter.github.io/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;<a class="more-link"
                      href="https://arxiv.org/abs/2310.11440" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/evalcrafter/EvalCrafter" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/evalcrafter/EvalCrafter" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/evalcrafter/EvalCrafter?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ ScaleCrafter ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://yingqinghe.github.io/scalecrafter/" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/scalecrafter.jpg" alt="teaser" />
                </a>

                <div class="desc col-md-8 col-12">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2310.07702" target="_blank">ScaleCrafter: Tuning-free
                      Higher-Resolution Visual Generation with Diffusion Models</a>
                  </h3>
                  <p class="mb-2">
                    Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, <b>Xintao Wang</b>,
                    Ran He, Qifeng Chen, Ying Shan
                  </p>
                  <p>
                    arXiv preprint: 2310.07702. <br>
                    ICLR, 2024 (<font color="red">spotlight</font>) &nbsp;
                    <a class="more-link" href="https://yingqinghe.github.io/scalecrafter/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2310.07702" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/YingqingHe/ScaleCrafter" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/YingqingHe/ScaleCrafter" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/YingqingHe/ScaleCrafter?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ SEED-LlaMa ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://ailab-cvc.github.io/seed/seed_llama.html" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/seed-llama.jpg" alt="teaser" />
                </a>

                <div class="desc col-md-8 col-12">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2310.01218" target="_blank">Making LLaMA SEE and Draw with SEED
                      Tokenizer</a>
                  </h3>
                  <p class="mb-2">
                    Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, <b>Xintao Wang</b>, Ying Shan
                    Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, , Ying Shan, Ziwei Liu
                  </p>
                  <p>
                    arXiv preprint: 2310.01218. <br>
                    ICLR, 2024 &nbsp;
                    <a class="more-link" href="https://ailab-cvc.github.io/seed/seed_llama.html" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2310.01218" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/AILab-CVC/SEED" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/AILab-CVC/SEED" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/AILab-CVC/SEED?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ StyleCrafter ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://arxiv.org/abs/2309.01770" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/styleadapter.jpg" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h4 class="title">
                    <a href="https://arxiv.org/abs/2309.01770" target="_blank">StyleAdapter: A Unified Stylized Image
                      Generation Model</a>
                  </h4>
                  <p class="mb-2">
                    Zhouxia Wang, <b>Xintao Wang<sup>#</sup></b>, Liangbin Xie, Zhongang Qi, Ying Shan, Wenping Wang,
                    Ping Luo<sup>#</sup>
                  </p>
                  <p>arXiv preprint: 2309.01770</br>
                    IJCV, 2024. &nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2309.01770" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                  </p>
                </div>
              </div>
              <!------------------------------------------ DragonDiffusion ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://mc-e.github.io/project/DragonDiffusion/" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/dragondiffusion.jpg"
                    alt="teaser" />
                </a>

                <div class="desc col-md-8 col-12">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2307.02421" target="_blank">DragonDiffusion: Enabling Drag-style
                      Manipulation on
                      Diffusion Models</a>
                  </h3>
                  <p class="mb-2">
                    Chong Mou, <b>Xintao Wang</b>, Jiechong Song, Ying Shan,
                    Jian Zhang
                  </p>
                  <p>
                    arXiv preprint: 2307.02421. <br>
                    ICLR, 2024 (<font color="red">spotlight</font>) &nbsp;
                    <a class="more-link" href="https://mc-e.github.io/project/DragonDiffusion/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2307.02421" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/MC-E/DragonDiffusion" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/MC-E/DragonDiffusion" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/MC-E/DragonDiffusion?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ DreamDiffusion ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://arxiv.org/abs/2306.16934" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/dreamdiffusion.jpg"
                    alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h4 class="title">
                    <a href="https://arxiv.org/abs/2306.16934" target="_blank">DreamDiffusion: Generating High-Quality
                      Images from Brain EEG Signals</a>
                  </h4>
                  <p class="mb-2">
                    Yunpeng Bai, <b>Xintao Wang</b>, Yan-Pei Cao, Yixiao Ge, Chun Yuan, Ying Shan
                  </p>
                  <p>arXiv preprint: 2306.16934 <br>
                    ECCV, 2024. &nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2306.16934" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/bbaaii/DreamDiffusion" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/bbaaii/DreamDiffusion" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/bbaaii/DreamDiffusion?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ MasaCtrl ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://ljzycmd.github.io/projects/MasaCtrl/" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/masactrl.png" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2304.08465" target="_blank">
                      <font color="orange">MasaCtrl: Tuning-Free Mutual Self-Attention Control for
                        Consistent Image Synthesis and Editing</font>
                    </a>
                  </h3>
                  <p class="mb-2">
                    Mingdeng Cao, <b>Xintao Wang<sup>#</sup></b>, Zhongang Qi, Ying Shan, Xiaohu Qie, Yinqiang
                    Zheng<sup>#</sup>
                  </p>
                  <p>arXiv preprint: 2304.08465</br>
                    ICCV, 2023. &nbsp;
                    <a class="more-link" href="https://ljzycmd.github.io/projects/MasaCtrl" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2304.08465" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/MasaCtrl" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/MasaCtrl" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/TencentARC/MasaCtrl?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ FollowYourPose ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://follow-your-pose.github.io/" target="_blank">
                  <video autoplay controls="" muted loop playsinline="" width="100%" style="width: 260px">
                    <source src="./images/follow-your-pose.mp4" type="video/mp4" />
                  </video>
                </a>
                <div class="desc col-md-8 col-12">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2304.01186" target="_blank">Follow Your Pose: Pose-Guided
                      Text-to-Video Generation
                      using Pose-Free Videos</a>
                  </h3>
                  <p class="mb-2">
                    Yue Ma, Yingqing He, Xiaodong Cun, <b>Xintao Wang</b>,
                    Ying Shan, Xiu Li, Qifeng Chen
                  </p>
                  <p>
                    arXiv preprint: 2304.01186.<br>
                    AAAI, 2024. &nbsp;
                    <a class="more-link" href="https://follow-your-pose.github.io/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2304.01186" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/mayuelala/FollowYourPose" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/mayuelala/FollowYourPose" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/mayuelala/FollowYourPose?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ FateZero ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://fate-zero-edit.github.io/" target="_blank">
                  <video autoplay controls="" muted loop playsinline="" width="100%" style="width: 260px">
                    <source src="./images/fatezero.mp4" type="video/mp4" />
                  </video>
                </a>
                <div class="desc col-md-8 col-12">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2303.09535" target="_blank">FateZero: Fusing Attentions for Zero-shot
                      Text-based
                      Video Editing</a>
                  </h3>
                  <p class="mb-2">
                    Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,
                    <b>Xintao Wang</b>, Ying Shan, Qifeng Chen
                  </p>
                  <p>arXiv preprint: 2303.09535</br>
                    ICCV, 2023 (<font color="red">oral</font>). &nbsp;
                    <a class="more-link" href="https://fate-zero-edit.github.io/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2303.09535" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/ChenyangQiQi/FateZero" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/ChenyangQiQi/FateZero" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/ChenyangQiQi/FateZero?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ T2I-Adapter ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://github.com/TencentARC/T2I-Adapter" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/t2i-adapter.jpg" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h4 class="title">
                    <a href="https://arxiv.org/abs/2302.08453" target="_blank">
                      <font color="orange">T2I-Adapter: Learning Adapters to Dig out More
                        Controllable Ability for Text-to-Image Diffusion
                        Models</font>
                    </a>
                  </h4>
                  <p class="mb-2">
                    Chong Mou, <b>Xintao Wang<sup>#</sup></b>, Liangbin Xie, Yanze Wu, Jian Zhang<sup>#</sup>,
                    Zhongang Qi, Ying Shan, Xiaohu Qie
                  </p>
                  <p>arXiv preprint: 2302.08453</br>
                    AAAI, 2023. &nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2302.08453" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/T2I-Adapter" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/T2I-Adapter" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/TencentARC/T2I-Adapter?style=social" /></a>
                  </p>
                </div>
              </div>

              <!-- ################################ 2022 ################################# -->
              <h4 class="title">
                <font color="#49ac43">2022</font>
              </h4>
              <!------------------------------------------ Tune-A-Video ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://tuneavideo.github.io" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/tuneavideo.gif" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2212.11565" target="_blank">
                      <font color="orange">Tune-A-Video: One-Shot Tuning of Image Diffusion Models
                        for Text-to-Video Generation</font>
                    </a>
                  </h3>
                  <p class="mb-2">
                    Jay Zhangjie Wu, Yixiao Ge, <b>Xintao Wang</b>, Weixian
                    Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu
                    Qie, Mike Zheng Shou
                  </p>
                  <p>arXiv preprint: 2212.11565</br>
                    ICCV, 2023. &nbsp;
                    <a class="more-link" href="https://tuneavideo.github.io/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2212.11565" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/showlab/Tune-A-Video" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/showlab/Tune-A-Video" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/showlab/Tune-A-Video?style=social" /></a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ DeSRA ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://github.com/TencentARC/DeSRA" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/DeSRA.jpg" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2307.02457" target="_blank">DeSRA: Detect and Delete the Artifacts of
                      GAN-based
                      Real-World Super-Resolution Models</a>
                  </h3>
                  <p class="mb-2">
                    Liangbin Xie*, <b>Xintao Wang*</b>, Xiangyu Chen*, Gen Li,
                    Ying Shan, Jiantao Zhou, Chao Dong
                  </p>
                  <p>
                    ICML, 2023. &nbsp;
                    <!-- <a class="more-link" href="https://fate-zero-edit.github.io/" target="_blank"><i class="fas fa-external-link-alt"></i>Project Page</a>&nbsp; -->
                    <a class="more-link" href="https://arxiv.org/abs/2307.02457" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/DeSRA" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/DeSRA" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/TencentARC/DeSRA?style=social" /></a>
                  </p>
                </div>
              </div>

              <!------------------------------------------ Dream3D ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://bluestyle97.github.io/dream3d/" target="_blank">
                  <video autoplay="" controls="" muted="" loop="" playsinline="" width="100%" style="width: 240px">
                    <source src="./images/dream3d_car.mp4" type="video/mp4" />
                  </video>
                </a>
                <div class="desc col-md-8 col-12">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2212.14704" target="_blank">Dream3D: Zero-Shot Text-to-3D Synthesis
                      Using 3D Shape
                      Prior and Text-to-Image Diffusion Models</a>
                  </h3>
                  <p class="mb-2">
                    Jiale Xu, <b>Xintao Wang<sup>#</sup></b>, Weihao Cheng, Yan-Pei Cao, Ying Shan, Xiaohu Qie,
                    Shenghua Gao<sup>#</sup>
                  </p>
                  <p>
                    CVPR, 2023. &nbsp;
                    <a class="more-link" href="https://bluestyle97.github.io/dream3d" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2212.14704" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="" target="_blank"><i class="fab fa-github"></i>Codes (Coming
                      Soon)</a>&nbsp;
                    <!-- <a class="more-link" href="【】" target="_blank"><img alt="GitHub stars" align="right"
                                            src="https://img.shields.io/github/stars/TencentARC/T2I-Adapter?style=social"></a> -->
                  </p>
                </div>
              </div>
              <!------------------------------------------ Rethinking VQ Tokenizer ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://arxiv.org/abs/2212.03185" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/rethink_vq_tokenizer.jpg"
                    alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h4 class="title">
                    <a href="https://arxiv.org/abs/2412.07760" target="_blank">
                      <font color="orange">Rethinking the Objectives of Vector-Quantized Tokenizers for Image Synthesis
                      </font>
                    </a>
                  </h4>
                  <p class="mb-2">
                    Yuchao Gu, <b>Xintao Wang</b>, Yixiao Ge, Ying Shan, Mike Zheng Shou
                  </p>
                  <p>arXiv preprint: 2212.03185</br>
                    CVPR, 2024. &nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2212.03185" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ OSRT ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://github.com/Fanghua-Yu/OSRT" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/osrt.jpg" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2302.03453" target="_blank">OSRT: Omnidirectional Image
                      Super-Resolution with
                      Distortion-aware Transformer</a>
                  </h3>
                  <p class="mb-2">
                    Fanghua Yu*, <b>Xintao Wang*</b>, Mingdeng Cao, Gen Li,
                    Ying Shan, Chao Dong<sup>#</sup>
                  </p>
                  <p>
                    CVPR, 2023. &nbsp;
                    <!-- <a class="more-link" href="【】" target="_blank"><i class="fas fa-external-link-alt"></i>Project Page</a>&nbsp; -->
                    <a class="more-link" href="https://arxiv.org/abs/2302.03453" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/Fanghua-Yu/OSRT" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/Fanghua-Yu/OSRT" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/Fanghua-Yu/OSRT?style=social" /></a>
                  </p>
                </div>
              </div>

              <!------------------------------------------ HAT ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://github.com/XPixelGroup/HAT" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/HAT.png" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2205.04437" target="_blank"><b>HAT</b>: Activating More Pixels in
                      Image
                      Super-Resolution Transformer</a>
                  </h3>
                  <p class="mb-2">
                    Xiangyu Chen, <b>Xintao Wang</b>,
                    <a href="https://scholar.google.com.hk/citations?user=mcROAxAAAAAJ&hl=en" target="_blank">Jiantao
                      Zhou</a>,
                    <a href="https://scholar.google.com.hk/citations?user=OSDCB0UAAAAJ" target="_blank">Chao Dong</a>
                  </p>
                  <p>
                    CVPR, 2023. &nbsp;
                    <!-- <a class="more-link" href="【】" target="_blank"><i class="fas fa-external-link-alt"></i>Project Page</a>&nbsp; -->
                    <a class="more-link" href="https://arxiv.org/abs/2205.04437" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/XPixelGroup/HAT" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/XPixelGroup/HAT" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/XPixelGroup/HAT?style=social" /></a>
                  </p>
                </div>
              </div>

              <!------------------------------------------ FastRealVSR ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://github.com/TencentARC/FastRealVSR" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/FastRealVSR.jpg" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2212.07339" target="_blank">Mitigating Artifacts in Real-World Video
                      Super-Resolution Models</a>
                  </h3>
                  <p class="mb-2">
                    Liangbin Xie, <b>Xintao Wang</b>, Shuwei Shi, Jinjin Gu,
                    Chao Dong, Ying Shan
                  </p>
                  <p>
                    AAAI, 2022. &nbsp;
                    <!-- <a class="more-link" href="https://arxiv.org/abs/2212.07339" target="_blank"><i class="fas fa-external-link-alt"></i>Project Page</a>&nbsp; -->
                    <a class="more-link" href="https://arxiv.org/abs/2212.07339" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/FastRealVSR" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/FastRealVSR" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/TencentARC/FastRealVSR?style=social" /></a>
                  </p>
                </div>
              </div>

              <!------------------------------------------ Accelerate ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://arxiv.org/abs/2205.05069" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/EfficientVSRTraining.png"
                    alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2205.05069" target="_blank">Accelerating the Training of Video
                      Super-resolution
                      Models</a>
                  </h3>
                  <p class="mb-2">
                    Lijian Lin, <b>Xintao Wang<sup>#</sup></b>, Zhongang Qi, Ying Shan
                  </p>
                  <p>
                    AAAI, 2022. &nbsp;
                    <!-- <a class="more-link" href="https://arxiv.org/abs/2212.07339" target="_blank"><i class="fas fa-external-link-alt"></i>Project Page</a>&nbsp; -->
                    <a class="more-link" href="https://arxiv.org/abs/2205.05069" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/Efficient-VSR-Training" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/Efficient-VSR-Training"
                      target="_blank"><img alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/TencentARC/Efficient-VSR-Training?style=social" /></a>
                  </p>
                </div>
              </div>

              <!------------------------------------------ AnimeSR ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://github.com/TencentARC/AnimeSR" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/animesr.jpg" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2206.07038" target="_blank">AnimeSR: Learning Real-World
                      Super-Resolution Models
                      for Animation Videos</a>
                  </h3>
                  <p class="mb-2">
                    Yanze Wu*, <b>Xintao Wang*</b>, Gen Li,
                    <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en" target="_blank">Ying Shan</a>
                  </p>
                  <p>
                    NeurIPS, 2022. &nbsp;
                    <!-- <a class="more-link" href="【】" target="_blank"><i class="fas fa-external-link-alt"></i>Project Page</a>&nbsp; -->
                    <a class="more-link" href="https://arxiv.org/abs/2206.07038" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/AnimeSR" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/AnimeSR" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/TencentARC/AnimeSR?style=social" /></a>
                  </p>
                </div>
              </div>

              <!------------------------------------------ Rethinking Alignment ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://github.com/XPixelGroup/RethinkVSRAlignment" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/rethink_alignment.jpg"
                    alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2207.08494" target="_blank">Rethinking Alignment in Video
                      Super-Resolution
                      Transformers</a>
                  </h3>
                  <p class="mb-2">
                    Shuwei Shi, Jinjin Gu, Liangbin Xie, <b>Xintao Wang</b>,
                    Yujiu Yang, Chao Dong
                  </p>
                  <p>
                    NeurIPS, 2022. &nbsp;
                    <!-- <a class="more-link" href="【】" target="_blank"><i class="fas fa-external-link-alt"></i>Project Page</a>&nbsp; -->
                    <a class="more-link" href="https://arxiv.org/abs/2207.08494" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/XPixelGroup/RethinkVSRAlignment" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/XPixelGroup/RethinkVSRAlignment" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/XPixelGroup/RethinkVSRAlignment?style=social" /></a>
                  </p>
                </div>
              </div>

              <!------------------------------------------ VQFR ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://github.com/TencentARC/VQFR" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/VQFR.png" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2205.06803" target="_blank">VQFR: Blind Face Restoration with
                      Vector-Quantized
                      Dictionary and Parallel Decoder</a>
                  </h3>
                  <p class="mb-2">
                    Yuchao Gu, <b>Xintao Wang</b>, Liangbie Xie, Chao Dong,
                    Gen Li, Ying Shan, Ming-Ming Cheng
                  </p>
                  <p>
                    <b>Selected as oral (2.7%)</b><br />
                    ECCV, 2022. &nbsp;
                    <!-- <a class="more-link" href="【】" target="_blank"><i class="fas fa-external-link-alt"></i>Project Page</a>&nbsp; -->
                    <a class="more-link" href="https://arxiv.org/abs/2205.06803" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/VQFR" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/VQFR" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/TencentARC/VQFR?style=social" /></a>
                  </p>
                </div>
              </div>

              <!------------------------------------------ MMRealSR ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://github.com/TencentARC/MM-RealSR" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/MM-RealSR.png" alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2205.05065" target="_blank">MM-RealSR: Metric Learning based
                      Interactive Modulation
                      for Real-World Super-Resolution</a>
                  </h3>
                  <p class="mb-2">
                    Chong Mou, Yanze Wu, <b>Xintao Wang</b>, Chao Dong, Jian
                    Zhang, Ying Shan
                  </p>
                  <p>
                    ECCV, 2022. &nbsp;
                    <!-- <a class="more-link" href="【】" target="_blank"><i class="fas fa-external-link-alt"></i>Project Page</a>&nbsp; -->
                    <a class="more-link" href="https://arxiv.org/abs/2205.05065" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/MM-RealSR" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/MM-RealSR" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/TencentARC/MM-RealSR?style=social" /></a>
                  </p>
                </div>
              </div>

              <!-- ################################ 2021 ################################# -->
              <h4 class="title">
                <font color="#49ac43">2021</font>
              </h4>
              <!------------------------------------------ Real-ESRGAN ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="https://github.com/xinntao/Real-ESRGAN" target="_blank">
                  <img class="img-fluid project-image rounded shadow-sm" src="./images/realesrgan_rlt.jpg"
                    alt="teaser" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2107.10833" target="_blank">Real-ESRGAN: Training Real-World Blind
                      Super-Resolution
                      with Pure Synthetic Data</a>
                  </h3>
                  <p class="mb-2">
                    <b>Xintao Wang</b>,
                    <a href="https://liangbinxie.github.io/" target="_blank">Liangbie Xie</a>,
                    <a href="https://scholar.google.com.hk/citations?user=OSDCB0UAAAAJ" target="_blank">Chao Dong</a>,
                    <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en" target="_blank">Ying Shan</a>
                  </p>
                  <p>
                    ICCVW, 2021. &nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2107.10833" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/xinntao/Real-ESRGAN" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="【】" target="_blank"><img alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/xinntao/Real-ESRGAN?style=social" /></a>
                  </p>
                </div>
              </div>

              <!------------------------------------------ GFP-GAN ----------------------------------->
              <div class="item row">
                <a class="col-md-4 col-12" href="./projects/gfpgan.html" target="_blank">
                  <!-- <img class="img-fluid project-image rounded shadow-sm" src="【image】" alt="teaser" /> -->
                  <img class="img-fluid project-image rounded shadow-sm" src="images/gfpgan_lr.jpg" width="350px"
                    onmouseover="this.src='images/gfpgan_hr.jpg'" width="350px"
                    onmouseout="this.src='images/gfpgan_lr.jpg'" width="350px" />
                </a>
                <div class="desc col-md-8 col-12">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2101.04061" target="_blank"><b>GFPGAN</b>: Towards Real-World Blind
                      Face
                      Restoration with Generative Facial Prior</a>
                  </h3>
                  <p class="mb-2">
                    <b>Xintao Wang</b>,
                    <a href="https://yu-li.github.io" target="_blank">Yu Li</a>,
                    <a href="https://scholar.google.com/citations?hl=en&user=KjQLROoAAAAJ" target="_blank">Honglun
                      Zhang</a>,
                    <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en" target="_blank">Ying Shan</a>
                  </p>
                  <p>
                    CVPR, 2021. &nbsp;
                    <a class="more-link" href="./projects/gfpgan.html" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2101.04061" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/GFPGAN" target="_blank"><i
                        class="fab fa-github"></i>Codes</a>&nbsp;
                    <a class="more-link" href="https://github.com/TencentARC/GFPGAN" target="_blank"><img
                        alt="GitHub stars" align="right"
                        src="https://img.shields.io/github/stars/TencentARC/GFPGAN?style=social" /></a>
                  </p>
                </div>
              </div>

              <!-- ################################ 2020 and before ################################# -->
              <h4 class="title">
                <font color="#49ac43">2020 and before</font>
              </h4>
              <font color="#49ac43"><b>To be updated</b></font>
            </div>
            <!--//content-->
          </div>
          <!--//section-inner-->
        </section>
        <!--//section-->
      </div>
      <!--//primary-->
      <div class="secondary col-lg-4 col-12">
        <aside class="info aside section">
          <div class="section-inner shadow-sm rounded">
            <h2 class="heading sr-only">Basic Information</h2>
            <div class="content">
              <ul class="list-unstyled">
                <li>
                  <i class="fab fa-twitter"></i><span class="sr-only">Twitter:</span><a
                    href="https://twitter.com/xinntao">X (Twitter)</a>
                </li>
                <li>
                  <i class="fab fa-google"></i><span class="sr-only">Google Scholar:</span><a
                    href="https://scholar.google.com.hk/citations?user=FQgZpQoAAAAJ&hl=en">Google Scholar</a>
                </li>
                <li>
                  <i class="fas fa-envelope"></i><span class="sr-only">Email:</span><a
                    href="email.html">xintao.wang@outlook.com</a>
                </li>
                <li>
                  <i class="fab fa-github"></i><span class="sr-only">GitHub:</span><a
                    href="https://github.com/xinntao">GitHub</a>
                </li>
                <!-- <li><i class="fas fa-map-marker-alt"></i><span class="sr-only">Location:</span>Shenzhen, China</li> -->
                <!-- <li><i class="fas fa-link"></i><span class="sr-only">Website:</span><a href="#">https://www.website.com</a></li> -->
              </ul>
            </div>
            <!--//content-->
          </div>
          <!--//section-inner-->
        </aside>
        <!--//aside-->

        <aside class="education aside section">
          <div class="section-inner shadow-sm rounded">
            <h2 class="heading">Experience</h2>
            <div class="content">
              <div class="item">
                <h3 class="title">
                  <i class="fas fa-building"></i> [2024-present] Researcher
                </h3>
                <h4 class="university">
                  <a href="https://www.kuaishou.com/en" target="_blank">Kuaishou Technology</a>
                </h4>
              </div>
              <!--//item-->
              <div class="item">
                <h3 class="title">
                  <i class="fas fa-building"></i> [2020-2023] Researcher
                </h3>
                <h4 class="university">
                  <a href="https://www.tencent.com/en-us/" target="_blank">Tencent</a>
                </h4>
              </div>
              <!--//item-->
            </div>
            <!--//content-->
          </div>
          <!--//section-inner-->
        </aside>
        <!--//section-->

        <aside class="education aside section">
          <div class="section-inner shadow-sm rounded">
            <h2 class="heading">Education</h2>
            <div class="content">
              <div class="item">
                <h3 class="title">
                  <i class="fas fa-graduation-cap"></i> Ph.D.
                </h3>
                <h4 class="university">
                  <a href="http://mmlab.ie.cuhk.edu.hk/" target="_blank">Multimedia Laboratory (MMLab)</a>,<br />
                  <a href="https://www.cuhk.edu.hk/english/index.html" target="_blank">The Chinese University of Hong
                    Kong</a>, <span class="year">2016-2020</span>
                </h4>
              </div>
              <!--//item-->
              <div class="item">
                <h3 class="title">
                  <i class="fas fa-graduation-cap"></i> B.Eng.
                </h3>
                <h4 class="university">
                  <a href="https://www.zju.edu.cn/english/" target="_blank">Zhejiang University</a>, <span
                    class="year">2012-2016</span>
                </h4>
              </div>
              <!--//item-->
            </div>
            <!--//content-->
          </div>
          <!--//section-inner-->

          <script type="text/javascript" id="clustrmaps"
            src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=67did0CAdncq-M1USj88a8nvNyHJzr771w9jz5KE4sQ&co=2d78ad&ct=ffffff&cmo=3acc3a&cmn=ff5353">
          </script>
        </aside>
        <!--//section-->
      </div>
      <!--//secondary-->
    </div>
    <!--//row-->
  </div>
  <!--//masonry-->

  <!-- ******FOOTER****** -->
  <footer class="footer">
    <div class="container text-center">
      <!--/* This template is free as long as you keep the attribution link below. Thank you for your support. :) If you'd like to use the template without the attribution, you can buy the commercial license via our website: themes.3rdwavemedia.com */-->
      <small class="copyright">This template is modified from
        <a href="https://themes.3rdwavemedia.com/demo/bs5/developer/" target="_blank">Xiaoying Riley's
          project</a></small>
    </div>
    <!--//container-->
  </footer>
  <!--//footer-->

  <!-- Javascript -->
  <!-- <script>
      if (localStorage.getItem("darkSwitch") === null) {
        localStorage.setItem("darkSwitch", "dark");
      }
    </script> -->
  <script type="text/javascript" src="assets/plugins/popper.min.js"></script>
  <script type="text/javascript" src="assets/plugins/bootstrap/js/bootstrap.min.js"></script>
  <script type="text/javascript" src="assets/plugins/vanilla-rss/dist/rss.global.min.js"></script>
  <script type="text/javascript" src="assets/plugins/dark-mode-switch/dark-mode-switch.min.js"></script>
  <!-- custom js -->
  <script type="text/javascript" src="assets/js/main.js"></script>
</body>

</html>